{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Import and Install Dependencies"
      ],
      "metadata": {
        "id": "ueLETpXnGdD8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4fe4mF9nGa3b"
      },
      "outputs": [],
      "source": [
        "!pip install psycopg2-binary\n",
        "!pip install torch torchvision torchaudio\n",
        "!pip install sentence-transformers psycopg2-binary faiss-cpu\n",
        "!pip install language_tool_python\n",
        "!pip install spacy\n",
        "!python -m spacy download en_core_web_sm\n",
        "!pip install imageio\n",
        "!pip install python-dotenv\n"
        "!pip install moviepy\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import psycopg2\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import torch\n",
        "import numpy as np\n",
        "import faiss\n",
        "import language_tool_python\n",
        "import spacy\n",
        "import os\n",
        "from moviepy.editor import VideoFileClip, concatenate_videoclips\n",
        "from dotenv import load_dotenv"
      ],
      "metadata": {
        "id": "tN5vwMkUGwJA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Storing Word Embeddings using PostgreSQL database"
      ],
      "metadata": {
        "id": "OPFU0lCgHOn1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ROOT = '.'"
      ],
      "metadata": {
        "id": "Q0Iruh0nMhwT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kFVn7UjPGa3d"
      },
      "outputs": [],
      "source": [
        "# Load environment variables from .env\n",
        "load_dotenv()\n",
        "\n",
        "# Accessing individual variables\n",
        "db_host = os.getenv(\"DB_HOST\")\n",
        "db_name = os.getenv(\"DB_NAME\")\n",
        "db_user = os.getenv(\"DB_USER\")\n",
        "db_password = os.getenv(\"DB_PASSWORD\")\n",
        "db_port = os.getenv(\"DB_PORT\")\n",
        "\n",
        "# Set seeds for reproducibility\n",
        "seed = 42\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "\n",
        "# Establish the connection\n",
        "conn = psycopg2.connect(\n",
        "    host=db_host,\n",
        "    dbname=db_name,\n",
        "    user=db_user,\n",
        "    password=db_password,\n",
        "    port=db_port\n",
        ")\n",
        "cur = conn.cursor()\n",
        "\n",
        "# Create the database table if not exists\n",
        "create_table_query = '''\n",
        "CREATE TABLE IF NOT EXISTS word_embeddings (\n",
        "    id SERIAL PRIMARY KEY,\n",
        "    word TEXT NOT NULL,\n",
        "    embedding FLOAT8[]\n",
        ");\n",
        "'''\n",
        "cur.execute(create_table_query)\n",
        "conn.commit()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DXnR9XkoGa3f",
        "outputId": "7906187c-1b50-43a0-829f-33d41138b16f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2000\n"
          ]
        }
      ],
      "source": [
        "# Open the file in read mode\n",
        "with open(os.path.join(ROOT, 'words.txt'), 'r') as file:\n",
        "    # Read all lines and strip the newline character\n",
        "    words = [line.strip() for line in file]\n",
        "\n",
        "# Print the words to verify\n",
        "print(len(words))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KYQLpoBEGa3f"
      },
      "outputs": [],
      "source": [
        "# Load a pre-trained sentence transformer model\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# Generate embeddings for the words\n",
        "embeddings = model.encode(words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OrybNLgFGa3g"
      },
      "outputs": [],
      "source": [
        "# Insert words and their embeddings into the database\n",
        "for word, embedding in zip(words, embeddings):\n",
        "    # Convert embedding to a Python list of floats\n",
        "    embedding_list = embedding.tolist()\n",
        "\n",
        "    cur.execute(\n",
        "        \"INSERT INTO word_embeddings (word, embedding) VALUES (%s, %s)\",\n",
        "        (word, embedding_list)\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Convert Text to Sign Videos"
      ],
      "metadata": {
        "id": "pJyIDTBcLEyx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1 Grammer Correction"
      ],
      "metadata": {
        "id": "Df-ndrSELY0r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def correct_english_grammar(text):\n",
        "    tool = language_tool_python.LanguageTool('en-US')\n",
        "    matches = tool.check(text)\n",
        "    corrected_text = language_tool_python.utils.correct(text, matches)\n",
        "    return corrected_text\n"
      ],
      "metadata": {
        "id": "MKD01RUmJAOR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xzKgsimoGa3h",
        "outputId": "9c57e908-cf69-41a4-c52c-22c119d43338"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading LanguageTool 6.4: 100%|██████████| 246M/246M [01:12<00:00, 3.39MB/s] \n",
            "Unzipping C:\\Users\\islam\\AppData\\Local\\Temp\\tmpgj8wsyy2.zip to C:\\Users\\islam\\.cache\\language_tool_python.\n",
            "Downloaded https://www.languagetool.org/download/LanguageTool-6.4.zip to C:\\Users\\islam\\.cache\\language_tool_python.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I live in Taxis\n"
          ]
        }
      ],
      "source": [
        "text = \"I lives in Taxiss\"\n",
        "corrected_text = correct_english_grammar(text)\n",
        "print(corrected_text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2 Lemmatization"
      ],
      "metadata": {
        "id": "Xo-9apr9LmDF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_lemmatized_words(sentence):\n",
        "    nlp = spacy.load('en_core_web_sm')\n",
        "    doc = nlp(sentence)\n",
        "    lemmas = [token.lemma_ for token in doc if token.pos_ not in ['AUX', 'ADP', 'SYM']]\n",
        "    return lemmas\n"
      ],
      "metadata": {
        "id": "_OOqBDxOJFZz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.3 Semantic Search"
      ],
      "metadata": {
        "id": "kbn_UOJTLftc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_words_and_embeddings_from_db():\n",
        "    cur.execute(\"SELECT word, embedding FROM word_embeddings\")\n",
        "    records = cur.fetchall()\n",
        "    words, embeddings = zip(*[(record[0], np.array(record[1])) for record in records])\n",
        "    return words, embeddings"
      ],
      "metadata": {
        "id": "sIN3JUE3LLhp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tC8q6-T0Ga3g"
      },
      "outputs": [],
      "source": [
        "def semantic_search(query, top_k=5):\n",
        "    query_embedding = model.encode([query])\n",
        "    words, embeddings = get_words_and_embeddings_from_db()\n",
        "    embeddings = np.array(embeddings)\n",
        "\n",
        "    dimension = embeddings.shape[1]\n",
        "    index = faiss.IndexFlatL2(dimension)\n",
        "    index.add(embeddings)\n",
        "    distances, indices = index.search(query_embedding, top_k)\n",
        "\n",
        "    similarities = 1 - distances / 2\n",
        "\n",
        "    results = []\n",
        "    threshold = 0.75\n",
        "\n",
        "\n",
        "    print(f\"Semantic Results of {query} :\")\n",
        "    for sim, idx in zip(similarities[0], indices[0]):\n",
        "        if sim >= threshold:\n",
        "            print(words[idx])\n",
        "            results.append(words[idx])\n",
        "    if not results:\n",
        "        return [list(query.upper())]\n",
        "    return results\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def semantic_search_multiword_glosses(lemmas):\n",
        "    tokens = []\n",
        "    for lemma in lemmas:\n",
        "        results = semantic_search(lemma)\n",
        "        tokens.append(results[0])\n",
        "    return tokens\n"
      ],
      "metadata": {
        "id": "SrsV1JQ2Jh-B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.4 Videos Concatenation"
      ],
      "metadata": {
        "id": "-hLNR-lGLzXF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_video_paths(tokens):\n",
        "    VIDEOS_PATH = os.path.join(ROOT, 'videos')\n",
        "    LETTERS_PATH = os.path.join(VIDEOS_PATH, 'video_letters')\n",
        "    video_paths = []\n",
        "    for token in tokens:\n",
        "        paths_to_check = []\n",
        "        if isinstance(token, list):\n",
        "            paths_to_check = [os.path.join(LETTERS_PATH, letter + '.mp4') for letter in token]\n",
        "        else:\n",
        "            paths_to_check = [os.path.join(VIDEOS_PATH, token + '.mp4')]\n",
        "\n",
        "        for path in paths_to_check:\n",
        "            if os.path.exists(path):\n",
        "                video_paths.append(path)\n",
        "            else:\n",
        "                print(\"Path not found at\", path)\n",
        "    return video_paths"
      ],
      "metadata": {
        "id": "UzO5hJgxJkDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def concatenate_videos(video_paths, output_path):\n",
        "    video_clips = []\n",
        "    for video_path in video_paths:\n",
        "        clip = VideoFileClip(video_path)\n",
        "        video_clips.append(clip)\n",
        "\n",
        "    concatenated_clip = concatenate_videoclips(video_clips)\n",
        "    concatenated_clip.write_videofile(output_path, codec='libx264')\n",
        "\n"
      ],
      "metadata": {
        "id": "OPFUi3GnJjFO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conversion Pipeline"
      ],
      "metadata": {
        "id": "FIXN9dxPMoPU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = 'I went to school yesterday'\n",
        "output_path = os.path.join(ROOT, 'concatenated.mp4')\n",
        "\n",
        "corrected_sentence = correct_english_grammar(sentence)\n",
        "print(\"corrected: \", corrected_sentence)\n",
        "\n",
        "lemmas = get_lemmatized_words(corrected_sentence)\n",
        "print(\"lemmatized: \", lemmas)\n",
        "\n",
        "glosses = semantic_search_multiword_glosses(lemmas)\n",
        "print(\"glosses: \", glosses)\n",
        "\n",
        "gif_paths = get_video_paths(glosses)\n",
        "concatenate_videos(gif_paths, output_path)"
      ],
      "metadata": {
        "id": "OLk-6YZtMzrg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Close Database Connection"
      ],
      "metadata": {
        "id": "UMld44TBMBBn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qOKsfK3MGa3j"
      },
      "outputs": [],
      "source": [
        "# Close the connection\n",
        "cur.close()\n",
        "conn.close()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
