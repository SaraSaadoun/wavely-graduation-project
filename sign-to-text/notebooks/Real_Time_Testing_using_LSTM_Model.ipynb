{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qHrWtu_g6QS3"
      },
      "source": [
        "# 1. Import and Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5_cbaKPm6QS8"
      },
      "outputs": [],
      "source": [
        "!pip install tensorflow tensorflow-gpu opencv-python mediapipe sklearn matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o1wkQ3EswQmR"
      },
      "outputs": [],
      "source": [
        "!pip install mediapipe\n",
        "!pip install groq\n",
        "!pip install python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ASWKwNA66QTB"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import mediapipe as mp\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "from groq import Groq\n",
        "import os\n",
        "from dotenv import load_dotenv\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "MODELS_PATH = './../model_weights'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GHtX4qb66QTD"
      },
      "source": [
        "# 2. Keypoints using MP Holistic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tGSFMrfs6QTE"
      },
      "outputs": [],
      "source": [
        "mp_holistic = mp.solutions.holistic # Holistic model\n",
        "mp_drawing = mp.solutions.drawing_utils # Drawing utilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def mediapipe_detection(image, model):\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Convert to RGB\n",
        "    image.flags.writeable = False  # Image is no longer writeable\n",
        "    results = model.process(image)  # Make prediction\n",
        "    image.flags.writeable = True  # Image is writeable again\n",
        "    return results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Ztv5XP66QTH"
      },
      "outputs": [],
      "source": [
        "\n",
        "def draw_styled_landmarks(image, results):\n",
        "    # Draw hand connections on the black image\n",
        "    if results.left_hand_landmarks:\n",
        "        mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
        "                                  mp_drawing.DrawingSpec(color=(121, 22, 76), thickness=2, circle_radius=4),\n",
        "                                  mp_drawing.DrawingSpec(color=(250, 44, 250), thickness=2, circle_radius=2))\n",
        "    if results.right_hand_landmarks:\n",
        "        mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
        "                                  mp_drawing.DrawingSpec(color=(121, 22, 76), thickness=2, circle_radius=4),\n",
        "                                  mp_drawing.DrawingSpec(color=(250, 44, 250), thickness=2, circle_radius=2))\n",
        "        \n",
        "    mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_CONTOURS,\n",
        "                             mp_drawing.DrawingSpec(color=(80,110,10), thickness=1, circle_radius=1),\n",
        "                             mp_drawing.DrawingSpec(color=(80,256,121), thickness=1, circle_radius=1)\n",
        "                             )\n",
        "    # Draw pose connections\n",
        "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS,\n",
        "                             mp_drawing.DrawingSpec(color=(80,22,10), thickness=2, circle_radius=4),\n",
        "                             mp_drawing.DrawingSpec(color=(80,44,121), thickness=2, circle_radius=2)\n",
        "                             )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mELYofHu6QTI"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Extract Keypoints from Landmarks And Concatenate it in One Array\n",
        "def extract_keypoints(results):\n",
        "    \"\"\"\n",
        "    inputs: Resutls from MediaPipe Model\n",
        "    output: Concatenated Landmarks\n",
        "    \"\"\"\n",
        "    # pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33*4)\n",
        "    # face = np.array([[res.x, res.y, res.z] for res in results.face_landmarks.landmark]).flatten() if results.face_landmarks else np.zeros(468*3)\n",
        "    lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n",
        "    rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)\n",
        "    return np.concatenate([lh, rh])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s1Qu8QdF6QTP",
        "outputId": "182b41e1-9e77-4cac-c635-f4968bb12d07"
      },
      "outputs": [],
      "source": [
        "# Define our Actions\n",
        "actions = np.array(['drink','eat','friend','goodbye','hello','help','how are you','no','yes','please','sorry','thanks','cry','i','they','you','what','name','teacher','family','happy','love','sad','laugh','neighbor','ok','read','write','school'])\n",
        "# actions = np.array(['goodbye', 'hello', 'how are you','i','name','no','ok','thanks','what','yes','you'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgIh0wVE5sG9"
      },
      "source": [
        "# 3. Preprocess Data and Create Labels and Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2_xtAYbg5K7f"
      },
      "outputs": [],
      "source": [
        "# DataPath of Data\n",
        "label_map = {label:num for num, label in enumerate(actions)}\n",
        "label_map"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5svhLpQz6QUI"
      },
      "source": [
        "# 4. Load LSTM Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aAOPi50S6QTY",
        "outputId": "a182577a-9e7c-45b5-fdc1-d174bd4ef83d"
      },
      "outputs": [],
      "source": [
        "# Build a LSTM Model Arch\n",
        "timesteps=30; features=84\n",
        "model_lstm = Sequential()\n",
        "model_lstm.add(LSTM(64, return_sequences=True, activation='relu', input_shape=(timesteps,features))) # frames * Features\n",
        "model_lstm.add(LSTM(128, return_sequences=True, activation='relu'))\n",
        "model_lstm.add(LSTM(64, return_sequences=False, activation='relu'))\n",
        "model_lstm.add(Dense(64, activation='relu'))\n",
        "model_lstm.add(Dense(32, activation='relu'))\n",
        "model_lstm.add(Dense(len(actions), activation='softmax'))\n",
        "model_lstm.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZSNLw3XH6QUI"
      },
      "outputs": [],
      "source": [
        "\n",
        "model_lstm.load_weights(os.path.join(MODELS_PATH,'LSTM29.h5'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 5. LLM Sentence Formation "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "load_dotenv()\n",
        "\n",
        "API_KEY = os.getenv('API_TOKEN')\n",
        "\n",
        "client = Groq(api_key=API_KEY)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def form_sentence_using_llm(scrambledWords):\n",
        "    \n",
        "    contentMsg = f\"\"\"\n",
        "    Task: Rearrange the following scrambled words into a coherent sentence and not complete the sentence and give only one answer only. it is okay even if the sentence consist of one word only \n",
        "    Example 1:\n",
        "    Scrambled Words: [love - I - dogs]\n",
        "    Complete the sentence:\n",
        "    I love dogs.\n",
        "    Example 2:\n",
        "    Scrambled Words: [beautiful - is - today - weather - the]\n",
        "    Complete the sentence:\n",
        "    The weather is beautiful today.\n",
        "    Example 3:\n",
        "    Scrambled Words: [eat - dinner - we - can - together]\n",
        "    Complete the sentence:\n",
        "    We can eat dinner together.\n",
        "    Example 4:\n",
        "    Scrambled Words: [ok]\n",
        "    Complete the sentence:\n",
        "    Okay.\n",
        "    New Instance:\n",
        "    Scrambled Words: [{scrambledWords.replace(' ', ' - ')}]\n",
        "    Complete the sentence:\n",
        "    \"\"\"\n",
        "\n",
        "    completion = client.chat.completions.create(\n",
        "        model=\"mixtral-8x7b-32768\",\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": contentMsg\n",
        "            }\n",
        "        ],\n",
        "        temperature=0.08,\n",
        "        max_tokens=1024,\n",
        "        top_p=1,\n",
        "        stream=True,\n",
        "        stop=None,\n",
        "    )\n",
        "    formed_sentence = []\n",
        "    for chunk in completion:\n",
        "        if chunk.choices[0].delta.content not in [None,'']:\n",
        "            formed_sentence.append(chunk.choices[0].delta.content)\n",
        "    return ''.join(formed_sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "form_sentence_using_llm('what you name')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 6. Real Time with skeleton"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "sequence = []\n",
        "sentence = []\n",
        "predictions = []\n",
        "formed_sentance = ''\n",
        "threshold = 0.9\n",
        "\n",
        "cap = cv2.VideoCapture(0)\n",
        "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        results = mediapipe_detection(frame, holistic)\n",
        "        black_image = np.zeros(frame.shape, dtype=np.uint8)\n",
        "        draw_styled_landmarks(black_image, results)\n",
        "        if results.left_hand_landmarks or results.right_hand_landmarks:\n",
        "        # Create a black image\n",
        "\n",
        "            keypoints = extract_keypoints(results)\n",
        "\n",
        "            lh_rh = keypoints\n",
        "            # Remove z Axis From Landmarks\n",
        "            for z in range(2, lh_rh.shape[0], 3):\n",
        "                lh_rh[z] = None\n",
        "            # Remove NaN Data\n",
        "            lh_rh = lh_rh[np.logical_not(np.isnan(lh_rh))]\n",
        "            sequence.append(lh_rh)\n",
        "            sequence = sequence[-30:]\n",
        "\n",
        "            if len(sequence) == 30:\n",
        "                # Convert sequence to numpy array\n",
        "                sequence_array = np.array(sequence)\n",
        "\n",
        "                # Reshape sequence_array to match model input shape\n",
        "                sequence_array = np.expand_dims(sequence_array, axis=0)  # Add batch dimension\n",
        "                sequence_array = sequence_array.reshape((1, 30, -1))  # Reshape to (1, 30, features)\n",
        "                res = model_lstm.predict(sequence_array)[0]\n",
        "                # print(\"res\", res)\n",
        "                predictions.append(np.argmax(res))\n",
        "\n",
        "                if np.unique(predictions[-15:])[0] == np.argmax(res):\n",
        "                    if res[np.argmax(res)] > threshold:\n",
        "                        if len(sentence) > 0:\n",
        "                            if actions[np.argmax(res)] != sentence[-1]:\n",
        "                                sentence.append(actions[np.argmax(res)])\n",
        "                        else:\n",
        "                            sentence.append(actions[np.argmax(res)])\n",
        "\n",
        "                if len(sentence) > 5:\n",
        "                    sentence = sentence[-5:]\n",
        "\n",
        "        # black_image = prob_viz(res, actions, black_image, colors)\n",
        "\n",
        "        # Get frame height\n",
        "        # sequence = []\n",
        "        frame_height = black_image.shape[0]\n",
        "        formed_sentance = formed_sentance.split('.')[0]\n",
        "        if len(formed_sentance) == 1:\n",
        "            ''.join(formed_sentance)\n",
        "        else:\n",
        "            ' '.join(formed_sentance)\n",
        "        # Draw the rectangle at the bottom of the screen\n",
        "        cv2.rectangle(black_image, (0, frame_height - 40), (640, frame_height), (245, 117, 16), -1)\n",
        "        cv2.putText(black_image, ' '.join(sentence), (3, frame_height - 10),\n",
        "                    cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
        "\n",
        "        cv2.rectangle(black_image, (0, frame_height - 80), (640, frame_height - 40), (117, 245, 16), -1)\n",
        "        cv2.putText(black_image,formed_sentance, (10, frame_height - 50),\n",
        "                    cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
        "\n",
        "\n",
        "        cv2.imshow('OpenCV Feed', black_image)\n",
        "\n",
        "        key = cv2.waitKey(10)\n",
        "        if key & 0xFF == ord('q'):\n",
        "            break\n",
        "        elif key & 0xFF == ord('s'):\n",
        "            formed_sentance = form_sentence_using_llm(''.join(sentence))\n",
        "            print(formed_sentance)\n",
        "            sentence = []\n",
        "        elif key & 0xFF == ord('d'):\n",
        "            sequence = []\n",
        "            sentence = []\n",
        "            predictions = []\n",
        "            formed_sentance = ''\n",
        "            \n",
        "\n",
        "    cap.release()\n",
        "    cv2.destroyAllWindows()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
